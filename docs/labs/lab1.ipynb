{"cells":[{"cell_type":"markdown","metadata":{"id":"pj25pEd2iEmU"},"source":["### A Quick Guide to Setting Up Mase in Google Colab\n","\n","1. Ensure that your Colab notebook is configured to use a GPU as an accelerator. To do this, navigate to Edit > Notebook settings or Runtime > Change runtime type and select GPU as the Hardware accelerator.\n","\n","2. Generate a personal access token from your GitHub account and use it to clone the Mase repository.\n","\n","3. Install all the required packages as specified in the \"requirements.txt\" file.\n","\n","4. At this stage, you should be ready to run Mase! To confirm, execute the training Jupyter notebook code sample below. From the output, you should observe:\n","\n","`GPU available: True (cuda), used: True, TPU available: False, using: 0 TPU cores, IPU available: False, using: 0 IPUs, HPU available: False, using: 0 HPUs,`\n","\n","indicating that the GPU is actively utilized during training."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"59amyCS2h-Vr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.11.4\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pip in /home/wfp23/.local/lib/python3.11/site-packages (23.3.2)\n"]}],"source":["# Check the current python version (It should be using Python 3.10) and update pip to the latest version.\n","!python --version\n","!python -m pip install --user --upgrade pip"]},{"cell_type":"markdown","metadata":{"id":"lvdVS5Z0t0bu"},"source":["Generate a personal access token and replace YOUR PERSONAL ACCESS TOKEN with your token\n","1. Visit GitHub website and login to your account.\n","2. Go to Settings, navigate to \"<> Developer settings\" and then click on Personal access tokens.\n","3. Click on Generate new token button on top right corner of the page.\n","4. Click the repo checkbox under Select scopes."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ni4y-7_piboN"},"outputs":[],"source":["git_token = \"YOUR PERSONAL ACCESS TOKEN\"\n","short_code = \"YOUR SHORTCODE\""]},{"cell_type":"markdown","metadata":{"id":"aM7qmEXDuekB"},"source":["Clone the repository:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"aCV2xFK4ibfi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Too many arguments.\n","\n","usage: git clone [options] [--] <repo> [<dir>]\n","\n","    -v, --verbose         be more verbose\n","    -q, --quiet           be more quiet\n","    --progress            force progress reporting\n","    -n, --no-checkout     don't create a checkout\n","    --bare                create a bare repository\n","    --mirror              create a mirror repository (implies bare)\n","    -l, --local           to clone from a local repository\n","    --no-hardlinks        don't use local hardlinks, always copy\n","    -s, --shared          setup as shared repository\n","    --recursive           initialize submodules in the clone\n","    --recurse-submodules  initialize submodules in the clone\n","    --template <template-directory>\n","                          directory from which templates will be used\n","    --reference <repo>    reference repository\n","    -o, --origin <name>   use <name> instead of 'origin' to track upstream\n","    -b, --branch <branch>\n","                          checkout <branch> instead of the remote's HEAD\n","    -u, --upload-pack <path>\n","                          path to git-upload-pack on the remote\n","    --depth <depth>       create a shallow clone of that depth\n","    --single-branch       clone only one branch, HEAD or --branch\n","    --separate-git-dir <gitdir>\n","                          separate git dir from working tree\n","    -c, --config <key=value>\n","                          set config inside the new repository\n","\n"]}],"source":["!git clone https://{git_token}@github.com/DeepWok/mase.git"]},{"cell_type":"markdown","metadata":{"id":"HdfYEV9rukLi"},"source":["Create your own branch:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ulTPHAZZuQK0"},"outputs":[{"name":"stderr","output_type":"stream","text":["bash: line 2: cd: mase: No such file or directory\n","Already on 'lab1_'\n"]},{"name":"stdout","output_type":"stream","text":["M\tdocs/labs/lab1.ipynb\n","M\tmachop/chop/models/physical/__init__.py\n","M\tmachop/chop/models/physical/jet_substructure/__init__.py\n","M\tmlir-air-output/lab1/lab1.md\n"]}],"source":["%%bash\n","# Check if the branch exists. Branch exists, so checkout to that branch otherwise create a new one.\n","cd mase\n","if git show-ref --quiet --verify \"refs/heads/lab1_${short_code}\"; then\n","    # Branch exists, so checkout to that branch\n","    git checkout lab1_${short_code}\n","else\n","    # Branch doesn't exist, so create it\n","    git branch lab1_${short_code}\n","    git checkout lab1_${short_code}\n","fi"]},{"cell_type":"markdown","metadata":{"id":"xXIFKsFquqj9"},"source":["Install the required packages:"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rpkUPu15ibU5"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: './mase/machop/requirements.txt'\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!python -m pip install -r ./mase/machop/requirements.txt"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"v7HYwNPbia-L"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/wfp23/ADL/mase/machop\n"]}],"source":["# Change working directory to machop\n","%cd /home/wfp23/ADL/mase/machop"]},{"cell_type":"markdown","metadata":{"id":"3Aq2s9qNu0j4"},"source":["At this stage, you are ready to run Mase! To confirm, execute `./ch` using with the `--help` option which will print out a usage guide"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"Gryn8a559mH6"},"outputs":[{"name":"stdout","output_type":"stream","text":["usage: ch [--config PATH] [--task TASK] [--load PATH] [--load-type]\n","          [--batch-size NUM] [--debug] [--log-level]\n","          [--report-to {wandb,tensorboard}] [--seed NUM] [--quant-config TOML]\n","          [--training-optimizer TYPE] [--trainer-precision TYPE]\n","          [--learning-rate NUM] [--weight-decay NUM] [--max-epochs NUM]\n","          [--max-steps NUM] [--accumulate-grad-batches NUM]\n","          [--log-every-n-steps NUM] [--cpu NUM] [--gpu NUM] [--nodes NUM]\n","          [--accelerator TYPE] [--strategy TYPE] [--auto-requeue]\n","          [--github-ci] [--disable-dataset-cache] [--target STR]\n","          [--num-targets NUM] [--pretrained] [--max-token-len NUM]\n","          [--project-dir DIR] [--project NAME] [-h] [-V] [--info [TYPE]]\n","          action [model] [dataset]\n","\n","Chop is a simple utility, part of the MASE tookit, to train, test and\n","transform (i.e. prune or quantise) a supported model.\n","\n","main arguments:\n","  action                action to perform. One of\n","                        (train|test|transform|search)\n","  model                 name of a supported model. Required if configuration\n","                        NOT provided.\n","  dataset               name of a supported dataset. Required if configuration\n","                        NOT provided.\n","\n","general options:\n","  --config PATH         path to a configuration file in the TOML format.\n","                        Manual CLI overrides for arguments have a higher\n","                        precedence. Required if the action is transform.\n","                        (default: None)\n","  --task TASK           task to perform. One of (classification|cls|translatio\n","                        n|tran|language_modeling|lm) (default: classification)\n","  --load PATH           path to load the model from. (default: None)\n","  --load-type           the type of checkpoint to be loaded; it's disregarded\n","                        if --load is NOT specified. It is designed to and must\n","                        be used in tandem with --load. One of (pt|pl|mz|hf)\n","                        (default: mz)\n","  --batch-size NUM      batch size for training and evaluation. (default: 128)\n","  --debug               run the action in debug mode, which enables verbose\n","                        logging, custom exception hook that uses ipdb, and\n","                        sets the PL trainer to run in \"fast_dev_run\" mode.\n","                        (default: False)\n","  --log-level           verbosity level of the logger; it's only effective\n","                        when --debug flag is NOT passed in. One of\n","                        (debug|info|warning|error|critical) (default: info)\n","  --report-to {wandb,tensorboard}\n","                        reporting tool for logging metrics. One of\n","                        (wandb|tensorboard) (default: tensorboard)\n","  --seed NUM            seed for random number generators set via Pytorch\n","                        Lightning's seed_everything function. (default: 0)\n","  --quant-config TOML   path to a configuration file in the TOML format.\n","                        Manual CLI overrides for arguments have a higher\n","                        precedence. (default: None)\n","\n","trainer options:\n","  --training-optimizer TYPE\n","                        name of supported optimiser for training. One of\n","                        (adam|sgd|adamw) (default: adam)\n","  --trainer-precision TYPE\n","                        numeric precision for training. One of\n","                        (16-mixed|32|64|bf16) (default: 16-mixed)\n","  --learning-rate NUM   initial learning rate for training. (default: 1e-05)\n","  --weight-decay NUM    weight decay for training. (default: 0)\n","  --max-epochs NUM      maximum number of epochs for training. (default: 20)\n","  --max-steps NUM       maximum number of steps for training. A negative value\n","                        disables this option. (default: -1)\n","  --accumulate-grad-batches NUM\n","                        number of batches to accumulate gradients. (default:\n","                        1)\n","  --log-every-n-steps NUM\n","                        log every n steps. No logs if num_batches <\n","                        log_every_n_steps. (default: 50))\n","\n","runtime environment options:\n","  --cpu NUM, --num-workers NUM\n","                        number of CPU workers; the default varies across\n","                        systems and is set to os.cpu_count(). (default: 48)\n","  --gpu NUM, --num-devices NUM\n","                        number of GPU devices. (default: 1)\n","  --nodes NUM           number of nodes. (default: 1)\n","  --accelerator TYPE    type of accelerator for training. One of\n","                        (auto|cpu|gpu|mps) (default: auto)\n","  --strategy TYPE       type of strategy for training. One of\n","                        (auto|ddp|ddp_find_unused_parameters_true) (default:\n","                        auto)\n","  --auto-requeue        enable automatic job resubmission on SLURM managed\n","                        cluster. (default: False)\n","  --github-ci           set the execution environment to GitHub's CI pipeline;\n","                        it's used in the MASE verilog emitter transform pass\n","                        to skip simulations. (default: False)\n","  --disable-dataset-cache\n","                        disable caching of datasets. (default: False)\n","\n","hardware generation options:\n","  --target STR          target FPGA for hardware synthesis. (default:\n","                        xcu250-figd2104-2L-e)\n","  --num-targets NUM     number of FPGA devices. (default: 100)\n","\n","language model options:\n","  --pretrained          load pretrained checkpoint from\n","                        HuggingFace/Torchvision when initialising models.\n","                        (default: False)\n","  --max-token-len NUM   maximum number of tokens. A negative value will use\n","                        tokenizer.model_max_length. (default: 512)\n","\n","project options:\n","  --project-dir DIR     directory to save the project to. (default:\n","                        /home/wfp23/ADL/mase/mase_output)\n","  --project NAME        name of the project. (default: {MODEL-NAME}_{TASK-\n","                        TYPE}_{DATASET-NAME}_{TIMESTAMP})\n","\n","information:\n","  -h, --help            show this help message and exit\n","  -V, --version         show version and exit\n","  --info [TYPE]         list information about supported models or/and\n","                        datasets and exit. One of (all|model|dataset)\n","                        (default: all)\n","\n","Maintained by the DeepWok Lab. Raise issues at\n","https://github.com/JianyiCheng/mase-tools/issues\n"]}],"source":["!./ch --help"]},{"cell_type":"markdown","metadata":{"id":"AbScYlDwAnJm"},"source":["### Using Colab as Virtual Machine\n","\n","Throughout the exercises you will use Google Colab extensively as a virtual machine. There are several ways to interact with the virtual machine through a command-line-like interface.\n","\n","1. `!` Commands\n","\n","    You can run a standard shell command in a cell by simply appending `!` to the beginning of the command. This effectively launches a new shell and runs the specified command within it.\n","\n","    `!ls`\n","\n","2. Magic (`%`) Commands\n","\n","    Certain 'Magic Commands' are provided by the IPython kernel.\n","    \n","    The main difference between magic and `!` commands are that `!` commands do not affect the current 'state' of the notebook runtime.\n","\n","    For example, to change the current directory, use a magic command:\n","\n","    `%cd foo`\n","\n","    rather than a `!` command.\n","\n","    (`!cd foo` only changes the directory in the shell that it launches and not the notebook runtime)\n","\n","3. Bash Cell\n","\n","    To convert an entire cell to run shell commands, add `%%bash` to the beginning of the cell. This is useful if you need to run multiple shell commands together. Note each command will be executed as a `!` rather than a magic command."]},{"cell_type":"markdown","metadata":{"id":"QepjYXkj_A7l"},"source":["# Training your first network\n","\n","In this section, we are interested in training a small network and evaluate the trained network through the command line flow.\n","\n","\n","The dataset we look at is the Jet Substructure Classification (JSC) dataset.\n","\n","> [A bit of physics]\n","Jets are collimated showers of particles that result from the decay and hadronization of quarks q and gluons g.\n","At the Large Hadron Collider (LHC), due to the high collision energy, a particularly interesting jet signature emerges from overlapping quark-initiated showers produced in decays of heavy standard model particles.\n","It is the task of jet substructure to distinguish the various radiation profiles of these jets from backgrounds consisting mainly of quark (u, d, c, s, b) and gluon-initiated jets. The tools of jet substructure have been used to distinguish interesting jet signatures from backgrounds that have production rates hundreds of times larger than the signal.\n","\n","In short, the dataset contains inputs with a feature size of 16 and 5 output classes."]},{"cell_type":"markdown","metadata":{"id":"BL0v-dWH_PQI"},"source":["## The train command\n","\n","To train a network for the JSC dataset, you would need to run:\n","\n","```bash\n","# You will need to run this command\n","./ch train jsc-tiny jsc --max-epochs 10 --batch-size 256\n","```\n","\n","`--max-epochs` states the maximum epochs allowed to train, and `--batch-size` defines the batch size for training.\n","\n","You should see a print out of the training configuration in a table\n","\n","```bash\n","+-------------------------+--------------------------+-----------------+--------------------------+\n","| Name                    |         Default          | Manual Override |        Effective         |\n","+-------------------------+--------------------------+-----------------+--------------------------+\n","| task                    |      classification      |                 |      classification      |\n","| load_name               |           None           |                 |           None           |\n","| load_type               |            mz            |                 |            mz            |\n","| batch_size              |           128            |       256       |           256            |\n","| to_debug                |          False           |                 |          False           |\n","| log_level               |           info           |                 |           info           |\n","| seed                    |            0             |                 |            0             |\n","| training_optimizer      |           adam           |                 |           adam           |\n","| trainer_precision       |            32            |                 |            32            |\n","| learning_rate           |          1e-05           |                 |          1e-05           |\n","| weight_decay            |            0             |                 |            0             |\n","| max_epochs              |            20            |       10        |            10            |\n","| max_steps               |            -1            |                 |            -1            |\n","| accumulate_grad_batches |            1             |                 |            1             |\n","| log_every_n_steps       |            50            |                 |            50            |\n","| num_workers             |            16            |        0        |            0             |\n","| num_devices             |            1             |                 |            1             |\n","| num_nodes               |            1             |                 |            1             |\n","| accelerator             |           auto           |                 |           auto           |\n","| strategy                |           ddp            |                 |           ddp            |\n","| is_to_auto_requeue      |          False           |                 |          False           |\n","| github_ci               |          False           |                 |          False           |\n","| disable_dataset_cache   |          False           |                 |          False           |\n","| target                  |   xcu250-figd2104-2L-e   |                 |   xcu250-figd2104-2L-e   |\n","| num_targets             |           100            |                 |           100            |\n","| is_pretrained           |          False           |                 |          False           |\n","| max_token_len           |           512            |                 |           512            |\n","| project_dir             | /Users/aaron/Projects/ma |                 | /Users/aaron/Projects/ma |\n","|                         |   se-tools/mase_output   |                 |   se-tools/mase_output   |\n","| project                 |           None           |                 |           None           |\n","| model                   |           None           |    jsc-tiny     |         jsc-tiny         |\n","| dataset                 |           None           |       jsc       |           jsc            |\n","+-------------------------+--------------------------+-----------------+--------------------------+\n","```\n","\n","There is also a summary on the model\n","\n","```bash\n","  | Name      | Type               | Params\n","-------------------------------------------------\n","0 | model     | JSC_Tiny           | 127\n","1 | loss_fn   | CrossEntropyLoss   | 0\n","2 | acc_train | MulticlassAccuracy | 0\n","3 | acc_val   | MulticlassAccuracy | 0\n","4 | acc_test  | MulticlassAccuracy | 0\n","5 | loss_val  | MeanMetric         | 0\n","6 | loss_test | MeanMetric         | 0\n","-------------------------------------------------\n","127       Trainable params\n","0         Non-trainable params\n","127       Total params\n","0.001     Total estimated model params size (MB)\n","```"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0nH7zIJ9jACf"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: ./ch: No such file or directory\n"]}],"source":["# Train a model!\n","!./ch train jsc-tiny jsc --max-epochs 1 --batch-size 256"]},{"cell_type":"markdown","metadata":{"id":"_AiO9E5vQtfc"},"source":["### Retrieve the checkpoint and training log to your local device (important!)\n","\n","A checkpoint serves as an interim snapshot of a model's complete internal state, encompassing its weights, current learning rate, and more. This enables the framework to resume training from this specific point whenever necessary.\n","\n","MASE produces an output directory after running the training flow. The output directory is found at `./mase_output/<model>_<task>_<dataset>_<current_date>`.\n","This directory includes\n","* `hardware` - a directory for Verilog hardware generated for the trained model\n","* `software` - a directory for any software generated for the trained model, including checkpoints, MASE models as well as any generated logs.\n","\n","Google Colab does not persist any files generated between different sessions. To save any output generated by MASE, it is recommended to save the output to a folder on your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"dTnGWcp4BqiX"},"source":["First, 'mount' your Google Drive to the current runtime. This adds your Google Drive to the runtime's file system and allows you to read/write to it programmatically."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"orff0sksBqG0"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"FAt9fyEgBxKH"},"source":["Then, copy your output to a directory on your Google Drive. (Ensure you are copying the correct path!)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"9wz-MvPBCwwf"},"outputs":[{"name":"stderr","output_type":"stream","text":["mkdir: cannot create directory ‘/content’: Permission denied\n","cp: cannot stat ‘/content/mase/mase_output/jsc-tiny_classification_jsc_2024-01-08’: No such file or directory\n"]},{"ename":"CalledProcessError","evalue":"Command 'b'mkdir -p /content/drive/MyDrive/mase_output\\ncp -r /content/mase/mase_output/jsc-tiny_classification_jsc_2024-01-08 /content/drive/MyDrive/mase_output\\n'' returned non-zero exit status 1.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmkdir -p /content/drive/MyDrive/mase_output\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcp -r /content/mase/mase_output/jsc-tiny_classification_jsc_2024-01-08 /content/drive/MyDrive/mase_output\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/mase/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n","File \u001b[0;32m~/anaconda3/envs/mase/lib/python3.11/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/anaconda3/envs/mase/lib/python3.11/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n","\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'mkdir -p /content/drive/MyDrive/mase_output\\ncp -r /content/mase/mase_output/jsc-tiny_classification_jsc_2024-01-08 /content/drive/MyDrive/mase_output\\n'' returned non-zero exit status 1."]}],"source":["%%bash\n","mkdir -p /content/drive/MyDrive/mase_output\n","cp -r /content/mase/mase_output/jsc-tiny_classification_jsc_2024-01-08 /content/drive/MyDrive/mase_output"]},{"cell_type":"markdown","metadata":{"id":"nqT08w9Ekd6Z"},"source":["### Logging on tensorboard (Colab)\n","\n","For any training commands executed, a logging directory would be created and one can use [tensorboard](https://www.tensorflow.org/tensorboard) to check the training trajectory.\n","\n","On Colab, load the tensorboard extension into the notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLSfx7aWF6gg"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"markdown","metadata":{"id":"A44j_HsjGnHP"},"source":["Then, run tensorboard on the logs that were just generated."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtDokQLm_-X3"},"outputs":[],"source":["# Check if the file name and date is correct!\n","%tensorboard --logdir /content/mase/mase_output/jsc-tiny_classification_jsc_2023-01-08/software/tensorboard/lightning_logs/version_0"]},{"cell_type":"markdown","metadata":{"id":"OQG-F3filbC7"},"source":["### The test command\n","\n","Under the same folder ../mase_output/jsc-tiny_classification_jsc_2023-11-12/software, there are also saved checkpoint files for the trained models. These are basically the trained parameters of the model, one can find more detail on Pytorch model checkpointing here and Lightning checkpointing here.\n","\n",">  **Note:** You have the flexibility to test any previously saved checkpoint from your local machine by uploading it back to the sidebar under /mase_output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qSEElrZjiqY"},"outputs":[],"source":["# Check if the file name and date is correct!\n","!./ch test jsc-tiny jsc --load ../mase_output/jsc-tiny_classification_jsc_2023-01-08/software/training_ckpts/best.ckpt --load-type pl"]},{"cell_type":"markdown","metadata":{"id":"8M6HgtlxmMDq"},"source":["The above command would return you the performance of the trained model on the test set. --load-type pl tells Machop that the checkpoint is saved by PyTorch Lightning. For PyTorch Lightning, see this [section](https://github.com/JianyiCheng/mase-tools/blob/coursework-prep/docs/labs/lab1.md#the-entry-point-for-the-train-action). The saved checkpoint can also be used to resume training."]},{"cell_type":"markdown","metadata":{"id":"u7fTNH9XjTLv"},"source":["## The definition of the JSC dataset\n","\n","Datasets are defined in under the [dataset](../../machop/chop/dataset) folder in `chop`, one should take a look at the [\\_\\_init__.py](../../machop/chop/dataset/__init__.py) to understand how different datasets are declared. The JSC dataset is defined and detailed in [this file](../../machop/chop/dataset/physical/jsc.py#L142):\n","\n","```python\n","@add_dataset_info(\n","    name=\"jsc\",\n","    dataset_source=\"manual\",\n","    available_splits=(\"train\", \"validation\", \"test\"),\n","    physical_data_point_classification=True,\n","    num_classes=5,\n","    num_features=16,\n",")\n","class JetSubstructureDataset(Dataset):\n","    def __init__(self, input_file, config_file, split=\"train\"):\n","        super().__init__()\n","  ...\n","```\n","\n","The [decorator](https://book.pythontips.com/en/latest/decorators.html) (if you do not know what is a python decorator, click the link and learn) defines the dataset information required. The class object `JetSubstructureDataset` has `Dataset` being its parent class. If you are still concerned with your proficiency in OOP (object orientated programming), you should check this [link](https://book.pythontips.com/en/latest/classes.html)."]},{"cell_type":"markdown","metadata":{"id":"7VgCaTaEnJ8b"},"source":["## The definition of the JSC Tiny network\n","\n","The network definition can also be found in the [\\_\\_init__.py](../../machop/chop/models/physical/jet_substructure/__init__.py#32)\n","\n","```python\n","class JSC_Tiny(nn.Module):\n","    def __init__(self, info):\n","        super(JSC_Tiny, self).__init__()\n","        self.seq_blocks = nn.Sequential(\n","            # 1st LogicNets Layer\n","            nn.BatchNorm1d(16),  #  batch norm layer\n","            nn.Linear(16, 5),  # linear layer\n","        )\n","\n","    def forward(self, x):\n","        return self.seq_blocks(x)\n","```\n","\n","Network definitions in Pytorch normally contains two components: an `__init__` method and a `forward` method. Also all networks and custom layers in Pytorch has to be a subclass of  `nn.Module`.\n","The neural network layers are initialised in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method.\n","\n","`nn.Sequential` is a container used for wrapping a number of layers together, more information on this container can be found in this [link](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html).\n"]},{"cell_type":"markdown","metadata":{"id":"YG6cY-4tnVmf"},"source":["# Varying the parameters\n","\n","We have executed the following training command above:\n","\n","````bash\n","./ch train jsc-tiny jsc --max-epochs 10 --batch-size 256\n","````\n","\n","We can, apparently, tune a bunch of parameters, and the obvious ones to tune are\n","\n","* `batch-size`\n","* `max-epochs`\n","* `learning-rate`\n","\n","Tune these parameters by hand and answer the following questions:\n","\n","1. What is the impact of varying batch sizes and why?\n","2. What is the impact of varying maximum epoch number?\n","3. What is happening with a large learning rate and what is happening with a small learning rate and why? What is the relationship between learning rates and batch sizes?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejTs_Nykn3L5"},"outputs":[],"source":["\"\"\"What is the impact of varying batch sizes and why?\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KgdPh34Ln2YF"},"outputs":[],"source":["\"\"\"What is the impact of varying maximum epoch number?\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyaSYfElncGJ"},"outputs":[],"source":["\"\"\"What is happening with a large learning and what is happening with a small learning rate and why?\n","What is the relationship between learning rates and batch sizes?\"\"\""]},{"cell_type":"markdown","metadata":{"id":"0jX0n8ujoC98"},"source":["# A deeper dive into the framework\n","\n","When you execute `./ch`, what really happens is the [ch](../../machop/ch) file got executed and from the `import` you can tell it is calling into [cli.py](../../machop/chop/cli.py).\n"]},{"cell_type":"markdown","metadata":{"id":"AK3fBGJNiauB"},"source":["## The entry point for the train/teset action\n","\n","When you choose to execute `./ch train`, we are executing the train action, and invoking [train.py](../../machop/chop/actions/train.py). The entire training flow is orchestrated using [PyTorch Lightning](https://lightning.ai/), so that the detailed lightning related wrapping occurs in [jet_substructure.py](../../machop/chop/plt_wrapper/physical/jet_substructure.py). PyTorch Lightning's checkpointing callbacks saves the model parameters (`torch.nn.Module.state_dict()`), the optimizer states, and other hyper-parameters specified in `lightning.pl.LightningModule`, so that the training can be resumed from the last checkpoint. The saved checkpoint has extension `.ckpt`, this is why we have `--load-type pl` in the `./ch test` command.\n","\n","Test action has similar implementation based on PyTorch Lightning ([test.py](../../machop/chop/actions/test.py))\n"]},{"cell_type":"markdown","metadata":{"id":"d0oCx71hpZaM"},"source":["## The entry point for the model\n","\n","All models are defined in the [\\_\\_init__.py](../../machop/chop/models/__init__.py) under the model folder. The `get_model` function is called inside `actions` (such as `train`) to ping down different models."]},{"cell_type":"markdown","metadata":{"id":"eVsz-ICLpiB2"},"source":["## The entry point for the dataset\n","\n","Similar to the model definitions, all datasets are defined in the [\\_\\_init__.py](../../machop/chop/dataset/__init__.py) under the dataset folder.\n"]},{"cell_type":"markdown","metadata":{"id":"FW5lQZnrpkjt"},"source":["# Train your own network\n","\n","Now you are familiar with different components in the tool.\n","\n","4. Implement a network that has in total around 10x more parameters than the toy network.\n","5. Test your implementation and evaluate its performance."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"OK50i-biBcmV"},"outputs":[{"name":"stdout","output_type":"stream","text":["Seed set to 0\n","+-------------------------+--------------------------+-----------------+--------------------------+\n","| Name                    |         Default          | Manual Override |        Effective         |\n","+-------------------------+--------------------------+-----------------+--------------------------+\n","| task                    |      classification      |                 |      classification      |\n","| load_name               |           None           |                 |           None           |\n","| load_type               |            mz            |                 |            mz            |\n","| batch_size              |           \u001b[38;5;8m128\u001b[0m            |       256       |           256            |\n","| to_debug                |          False           |                 |          False           |\n","| log_level               |           info           |                 |           info           |\n","| report_to               |       tensorboard        |                 |       tensorboard        |\n","| seed                    |            0             |                 |            0             |\n","| quant_config            |           None           |                 |           None           |\n","| training_optimizer      |           adam           |                 |           adam           |\n","| trainer_precision       |         16-mixed         |                 |         16-mixed         |\n","| learning_rate           |          1e-05           |                 |          1e-05           |\n","| weight_decay            |            0             |                 |            0             |\n","| max_epochs              |            \u001b[38;5;8m20\u001b[0m            |       10        |            10            |\n","| max_steps               |            -1            |                 |            -1            |\n","| accumulate_grad_batches |            1             |                 |            1             |\n","| log_every_n_steps       |            50            |                 |            50            |\n","| num_workers             |            48            |                 |            48            |\n","| num_devices             |            1             |                 |            1             |\n","| num_nodes               |            1             |                 |            1             |\n","| accelerator             |           auto           |                 |           auto           |\n","| strategy                |           auto           |                 |           auto           |\n","| is_to_auto_requeue      |          False           |                 |          False           |\n","| github_ci               |          False           |                 |          False           |\n","| disable_dataset_cache   |          False           |                 |          False           |\n","| target                  |   xcu250-figd2104-2L-e   |                 |   xcu250-figd2104-2L-e   |\n","| num_targets             |           100            |                 |           100            |\n","| is_pretrained           |          False           |                 |          False           |\n","| max_token_len           |           512            |                 |           512            |\n","| project_dir             | /home/wfp23/ADL/mase/mas |                 | /home/wfp23/ADL/mase/mas |\n","|                         |         e_output         |                 |         e_output         |\n","| project                 |           None           |                 |           None           |\n","| model                   |           \u001b[38;5;8mNone\u001b[0m           |       jsc       |           jsc            |\n","| dataset                 |           \u001b[38;5;8mNone\u001b[0m           |     jsc-toy     |         jsc-toy          |\n","+-------------------------+--------------------------+-----------------+--------------------------+\n","\u001b[32mINFO    \u001b[0m \u001b[34mInitialising model 'jsc'...\u001b[0m\n","Traceback (most recent call last):\n","  File \"/home/wfp23/ADL/mase/machop/./ch\", line 6, in <module>\n","    ChopCLI().run()\n","    ^^^^^^^^^\n","  File \"/home/wfp23/ADL/mase/machop/chop/cli.py\", line 236, in __init__\n","    ) = self._setup_model_and_dataset()\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/wfp23/ADL/mase/machop/chop/cli.py\", line 742, in _setup_model_and_dataset\n","    dataset_info = get_dataset_info(self.args.dataset)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/home/wfp23/ADL/mase/machop/chop/dataset/__init__.py\", line 45, in get_dataset_info\n","    raise ValueError(f\"Dataset {name} is not supported\")\n","ValueError: Dataset jsc-toy is not supported\n"]}],"source":["\"\"\"\n","Implement a network that has in total around 10x more parameters than the toy network.\n","Test your implementation and evaluate its performance.\n","\"\"\"\n","# Example parameters:\n","dataset=\"jsc\"\n","model=\"jsc-toy\"\n","epochs=10\n","batch_size=256\n","\n","!./ch train $dataset $model --max-epochs $epochs --batch-size $batch_size"]},{"cell_type":"markdown","metadata":{"id":"rr-qYp8-WLT-"},"source":["### Navigating Git for Version Control for mase_tools\n","\n","For those unfamiliar with Git, here's a step-by-step guide on how to use it to keep track of your project's changes. The following commands help you manage your code effectively:\n","\n","1. **Verify Your Branch:**\n","   ```bash\n","   git branch # Check if you are on your own branch\n","   ```\n","   Ensure you're working on the correct branch to avoid unintended changes.\n","2. **Check File Status:**\n","   ```bash\n","   git status # Check the files and the changes you have made\n","   ```\n","   This command provides an overview of modified, untracked, or staged files.\n","\n","3. **Stage Changes:**\n","   ```bash\n","   git add . # Stage all changes for commit\n","   ```\n","   'Staging' prepares changes for a commit. The '.' denotes all changes.\n","\n","4. **Commit Changes:**\n","   ```bash\n","   git commit -m \"lab1_new_toy_model\" # Commit changes with a descriptive message E.g. lab1_new_toy_model here\n","   ```\n","   A commit is a snapshot of your changes. Include a clear message describing the changes made. E.g. lab1_new_toy_model here\n","\n","5. **Push to Remote Repository:**\n","   ```bash\n","   git push # Push changes to the remote repository\n","   ```\n","   Upload your committed changes to the shared repository, ensuring your branch is updated.\n","\n","Executing these commands helps maintain a structured version history of your project and facilitates collaboration with others. If you're new to Git, practice these steps to enhance your code management skills.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RaY2awa3C1Vh"},"outputs":[],"source":["!git status # Check the files and the changes you have made\n","!git branch # Check if you are on your own branch\n","#!git add . # Stage all changes for commit\n","#!git commit -m \"A descriptive message\" # Commit changes with a descriptive message\n","#!git push # Push changes to the remote repository"]},{"cell_type":"markdown","metadata":{"id":"meYpvLndZNIs"},"source":["### Ensure Your Notebook Aligns with the Latest Remote Changes!\n","\n","If you've committed and pushed changes to your dedicated branch before, it's essential to synchronize your local `mase_tools` folder with the most recent updates from the remote repository. Achieve this by running the following command:\n","\n","```bash\n","git pull # Fetch and incorporate the latest changes\n","```"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1W86mRxD6mU6kw_F89_-rYi9XvtOiqiG_","timestamp":1700047242436}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
